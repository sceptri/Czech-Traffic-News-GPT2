{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ac2d70-6791-4b1f-b65d-b8c88ce0eeba",
   "metadata": {},
   "source": [
    "# Train your own small GPT-2 model\n",
    "\n",
    "If you want to experiment with the trained model, you can do it at `Inference API` panel of\n",
    "\n",
    "https://huggingface.co/openai-community/gpt2?text=My+name+is+Thomas+and+my+main\n",
    "\n",
    "Note that we are training small GPT2 model on a tiny dataset. Still We can see observe how the model improve with the number of steps and get some interesting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df4ce989-ea7f-4c89-975c-046038996d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wandb  # we will talk about wandb next lecture\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef7cfa7-f664-4ea9-8f04-17ce270272de",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "Before training, we have to tokenize the data and split them into chunks of the same size as context size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ec1afff-194b-461c-8073-75b769ccc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your own dataset\n",
    "dataset = load_dataset(\"TomasHalmazna/czech_traffic_urls\")\n",
    "\n",
    "# Make validation split\n",
    "dataset = dataset['train'].train_test_split(test_size=0.0015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3e836a-8548-496b-bdaf-d2b98f5e2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the gpt-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82b1dfeb-3191-414e-b6b9-0b198946d334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2127719f6342c99a311d042e1576ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2546 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c938c183bb947ea8baf3529e789353e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1030\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(text=example[\"text\"])\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True, remove_columns='text')\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98cdb38c-eb4f-47e7-8c43-96bacb7826ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 2494\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def concatenate_and_chunk(dataset, chunk_size=512):\n",
    "    # Flatten all `input_ids` into a single list\n",
    "    all_input_ids = list(chain(*dataset[\"input_ids\"]))\n",
    "    \n",
    "    # Create chunks of `chunk_size`\n",
    "    chunks = [all_input_ids[i:i + chunk_size] for i in range(0, len(all_input_ids), chunk_size)]\n",
    "    \n",
    "    # Only keep chunks that are exactly of length `chunk_size`\n",
    "    chunks = [chunk for chunk in chunks if len(chunk) == chunk_size]\n",
    "    \n",
    "    # Create a new dataset with only the `input_ids` chunks\n",
    "    return Dataset.from_dict({\"input_ids\": chunks})\n",
    "\n",
    "# Apply this function to each split (train and test) in the DatasetDict\n",
    "chunked_ds = DatasetDict({\n",
    "    split: concatenate_and_chunk(split_ds, chunk_size=512)\n",
    "    for split, split_ds in tokenized_ds.items()\n",
    "})\n",
    "\n",
    "chunked_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f1ef42-f488-4ca0-af36-464f3e535be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collator joins chunks into batches\n",
    "# see https://huggingface.co/docs/transformers/en/main_classes/data_collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe26fe6-925f-49ec-a854-ebdb429c66ad",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37301b62-4bea-4006-aa1a-7785924227b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model configuration for the smallest GPT-2\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer),      # Standard GPT-2 vocab size 50257\n",
    "    n_positions=512,                # Context size (512 is enough for small-scale models)\n",
    "    n_embd=768,                     # Embedding size\n",
    "    n_layer=12,                     # Number of transformer layers\n",
    "    n_head=12,                      # Number of attention heads\n",
    ")\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7a48b42-9ce2-433f-acff-ce694449c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Define the perplexity metric\n",
    "def compute_metrics(eval_pred):\n",
    "    # `eval_pred` is a tuple of (logits, labels)\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Convert logits and labels to PyTorch tensors if they are NumPy arrays\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.tensor(logits)\n",
    "    if isinstance(labels, np.ndarray):\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "    # Shift labels so that tokens align for calculating loss\n",
    "    shift_labels = labels[:, 1:].reshape(-1)\n",
    "    shift_logits = logits[:, :-1, :].reshape(-1, logits.shape[-1])\n",
    "\n",
    "    # Calculate the cross-entropy loss\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)  # Ignore padding tokens\n",
    "    loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = math.exp(loss.item())\n",
    "    return {\"perplexity\": perplexity}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4c203-9d5d-494e-b425-4b5a265516ff",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1ff634b-e879-44ad-a6d1-21c0a573aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1951/2912953097.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model,\n"
     ]
    }
   ],
   "source": [
    "# Set this according to size of your dataset\n",
    "# You should train for at least 15 mins on A10 GPU to get something reasonable\n",
    "TRAIN_EPOCHS = 10\n",
    "\n",
    "SAVE_STEPS = 1000\n",
    "EVAL_STEPS = SAVE_STEPS // 2\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-training\",  # Directory to save the model checkpoints and other outputs\n",
    "    eval_strategy=\"steps\",  # Evaluation strategy to use during training ('steps' or 'epochs')\n",
    "    eval_steps=EVAL_STEPS,  # Perform evaluation every EVAL_STEPS steps\n",
    "    num_train_epochs=TRAIN_EPOCHS,  # Total number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size for training on each device\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation on each device\n",
    "    learning_rate=2.5e-4,  # Initial learning rate for the optimizer\n",
    "    lr_scheduler_type='cosine',  # Learning rate scheduler type. 'cosine' provides a cosine decay schedule.\n",
    "    warmup_ratio=0.05,  # Proportion of training to perform linear learning rate warmup for\n",
    "    adam_beta1=0.9,  # Beta1 parameter for the Adam optimizer (first moment decay)\n",
    "    adam_beta2=0.999,  # Beta2 parameter for the Adam optimizer (second moment decay)\n",
    "    weight_decay=0.01,  # Weight decay to apply (L2 regularization)\n",
    "    logging_strategy=\"steps\",  # Logging strategy to use. 'steps' logs at specified steps.\n",
    "    logging_steps=EVAL_STEPS,  # Log training metrics every EVAL_STEPS steps\n",
    "    save_steps=SAVE_STEPS,  # Save a checkpoint every SAVE_STEPS steps\n",
    "    save_total_limit=10,  # Maximum number of checkpoints to keep. Older checkpoints are deleted.\n",
    "    # report_to='wandb',  # Uncomment to report metrics to Weights and Biases (optional)\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                 args = training_args,\n",
    "                 tokenizer=tokenizer,\n",
    "                 train_dataset=chunked_ds[\"train\"],\n",
    "                 eval_dataset=chunked_ds[\"test\"],\n",
    "                 compute_metrics=compute_metrics,\n",
    "                 data_collator = data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f72a95fd-d7ac-4403-81d8-a42532f7a7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1560' max='1560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1560/1560 09:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.923200</td>\n",
       "      <td>2.703200</td>\n",
       "      <td>14.927190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.492100</td>\n",
       "      <td>2.104363</td>\n",
       "      <td>8.201702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.938000</td>\n",
       "      <td>2.005440</td>\n",
       "      <td>7.429206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1560, training_loss=2.74758233779516, metrics={'train_runtime': 543.0611, 'train_samples_per_second': 45.925, 'train_steps_per_second': 2.873, 'total_flos': 6516623278080000.0, 'train_loss': 2.74758233779516, 'epoch': 10.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54b62744-896e-4faf-b21b-3e031dc1217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./gpt2-small-final\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43693524",
   "metadata": {},
   "source": [
    "*Upload to HuggingFace*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01639456-2124-46e8-8d6c-46b4cbd5c484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf2bba663d0412088087aaefd3542e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b161efc21d4f7e96cea8f8bcba2d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/TomasHalmazna/my_small_gpt2_trains/commit/2b0b0e2139499dc79ce52c7005aaca367730e40d', commit_message='Upload tokenizer', commit_description='', oid='2b0b0e2139499dc79ce52c7005aaca367730e40d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/TomasHalmazna/my_small_gpt2_trains', endpoint='https://huggingface.co', repo_type='model', repo_id='TomasHalmazna/my_small_gpt2_trains'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_MODEL_NAME = \"my_small_gpt2_trains\" # change this\n",
    "HF_TOKEN = \"\"  # change this \n",
    "\n",
    "model.push_to_hub(YOUR_MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer.push_to_hub(YOUR_MODEL_NAME, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6dac4-f46e-42a2-833f-07cb16eab3bc",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now you can switch from GPU to CPU. Try to complete some prompt specific to your dataset.\n",
    "\n",
    "Does it make sense? Is it at least in Czech/Slovak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f9ade4a-f8b3-441d-8b6f-550048a21f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  GPT2LMHeadModel, AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ee0298-2d7f-4aa4-9a2d-13429b7e197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  GPT2LMHeadModel.from_pretrained(\"./gpt2-small-final\")\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e42697e6-10ad-4ea8-a9a9-addef655a1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Vlak jede z Prahy do Brna a v úseku Ostrava-Svinov – Bohumín. Do této trati je přes Vsetínský nad'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = \"Vlak jede\" # Set starting prompt, something specific for your dataset\n",
    "\n",
    "generator(\n",
    "    PROMPT,\n",
    "    max_length=50,       # Maximum length of the generated text\n",
    "    do_sample=True,\n",
    "    temperature=0.5,         # Experiment with this\n",
    "    repetition_penalty=1.9,  # Experiment with this\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b12a36-09d5-4e3b-99bd-1ea157012107",
   "metadata": {},
   "source": [
    "Now go back to your training folder `.gpt2-training/`. Each `checkpoint-N` folder contains the model saved after N steps. \n",
    "\n",
    "If you experiment with the older models, you should see that the models improves with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e7a20e6-c47b-44e5-8d1a-2e6825dbef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_after_N_steps(N, prompt, **kwargs):\n",
    "    model =  GPT2LMHeadModel.from_pretrained(f\"./gpt2-training/checkpoint-{N}/\")\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    output = generator(prompt, **kwargs)\n",
    "    return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fc850f1-9c94-4a71-b56b-a2eecc5eecf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Kolejeně. Důvodem je také př'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample_after_N_steps(1000, \"Koleje\", do_sample=True, temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef172d4e",
   "metadata": {},
   "source": [
    "**Summary of Our Model**\n",
    "\n",
    "- Our model is far from perfect. While the results it produces are in Czech and sometimes return a grammatically correct sentence, they usually do not make meaningful sense.\n",
    "- The issue lies in the limited word database it is based on and the small number of epochs. Unfortunately, we didn't have enough memory to train the model for more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f36d31",
   "metadata": {},
   "source": [
    "**HF Pre-trained models**\n",
    "- let us now take some model from the HuggingFace hub and use it to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "060dd3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([dataset[\"train\"][i]['text'] for i in range(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46059079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simpletransformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.70.1)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (1.26.4)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (4.66.5)\n",
      "Requirement already satisfied: regex in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (2024.9.11)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (4.46.2)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (3.0.2)\n",
      "Requirement already satisfied: scipy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (1.3.2)\n",
      "Requirement already satisfied: seqeval in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (1.2.2)\n",
      "Requirement already satisfied: tensorboard in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (2.15.1)\n",
      "Requirement already satisfied: tensorboardx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (2.6.2.2)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (2.1.4)\n",
      "Requirement already satisfied: tokenizers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (0.20.3)\n",
      "Requirement already satisfied: wandb>=0.10.32 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (0.18.6)\n",
      "Requirement already satisfied: streamlit in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (1.40.0)\n",
      "Requirement already satisfied: sentencepiece in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from simpletransformers) (0.2.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.31.0->simpletransformers) (0.4.5)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (4.23.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (6.0.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (2.18.0)\n",
      "Requirement already satisfied: setproctitle in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (72.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->simpletransformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->simpletransformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->simpletransformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->simpletransformers) (2024.8.30)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->simpletransformers) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->simpletransformers) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->simpletransformers) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->simpletransformers) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->simpletransformers) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets->simpletransformers) (3.10.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->simpletransformers) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->simpletransformers) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->simpletransformers) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn->simpletransformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn->simpletransformers) (3.5.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit->simpletransformers) (5.4.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit->simpletransformers) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit->simpletransformers) (5.5.0)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit->simpletransformers) (10.4.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit->simpletransformers) (13.8.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit->simpletransformers) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit->simpletransformers) (0.10.2)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit->simpletransformers) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit->simpletransformers) (6.4.1)\n",
      "Requirement already satisfied: watchdog<6,>=2.1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit->simpletransformers) (5.0.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.66.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (2.34.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (3.7)\n",
      "Requirement already satisfied: six>1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard->simpletransformers) (3.0.4)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.5.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (1.13.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.9.11)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->simpletransformers) (2.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.5)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.20.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->simpletransformers) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58116d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['News']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args= {\n",
    "            \"num_train_epochs\": 15,\n",
    "            \"learning_rate\": 1e-5,\n",
    "            \"max_seq_length\": 512,\n",
    "            \"silent\": True\n",
    "            }\n",
    "model = ClassificationModel(\n",
    "    \"xlmroberta\", \"classla/xlm-roberta-base-multilingual-text-genre-classifier\", use_cuda=True,\n",
    "    args=model_args\n",
    "    \n",
    ")\n",
    "predictions, logit_output = model.predict([[dataset[\"train\"][i]['text'] for i in range(2)]]\n",
    "                                        )\n",
    "predictions\n",
    "# Output: array([3, 8])\n",
    "\n",
    "[model.config.id2label[i] for i in predictions]\n",
    "# Output: ['Instruction', 'Promotion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c350bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b4b5e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:484: UserWarning: use_multiprocessing automatically disabled as xlmroberta fails when using multiprocessing for feature conversion.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zastupitelé Ústeckého kraje včera udělalil prv...</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ministerstva dopravy a životního prostředí fin...</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stavba posledního úseku D1 u Přerova přinesla ...</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Památkově chráněné Hranické viadukty čeká reko...</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>České dráhy završily podpisem smlouvy se stave...</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text predicted_label\n",
       "0  Zastupitelé Ústeckého kraje včera udělalil prv...            News\n",
       "1  Ministerstva dopravy a životního prostředí fin...            News\n",
       "2  Stavba posledního úseku D1 u Přerova přinesla ...            News\n",
       "3  Památkově chráněné Hranické viadukty čeká reko...            News\n",
       "4  České dráhy završily podpisem smlouvy se stave...            News"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define model arguments\n",
    "model_args = {\n",
    "    \"num_train_epochs\": 15,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"silent\": True\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = ClassificationModel(\n",
    "    \"xlmroberta\", \"classla/xlm-roberta-base-multilingual-text-genre-classifier\", use_cuda=True,\n",
    "    args=model_args\n",
    ")\n",
    "\n",
    "# Create an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop over each observation in the dataset and make predictions\n",
    "for i in range(100):\n",
    "    # Get the text for the current observation\n",
    "    text = dataset[\"train\"][i]['text']\n",
    "    \n",
    "    # Make a prediction for the current observation\n",
    "    predictions, _ = model.predict([text])\n",
    "    \n",
    "    # Get the label for the prediction\n",
    "    label = model.config.id2label[predictions[0]]\n",
    "    \n",
    "    # Append the result to the list\n",
    "    results.append({\"text\": text, \"predicted_label\": label})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "output = pd.DataFrame(results)\n",
    "\n",
    "# Display or save the output dataset\n",
    "output.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dbf113",
   "metadata": {},
   "source": [
    "We can see that the most ou our sentences are \"news\". We can save the output and check whether there is some other genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc09e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save labes as csv\n",
    "output[['predicted_label']].to_csv(\"output_labels.csv\", index=False, header=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60c7c961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Opinion/Argumentation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Information/Explanation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     label  count\n",
       "0                     News     96\n",
       "1                    Other      2\n",
       "2    Opinion/Argumentation      1\n",
       "3  Information/Explanation      1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = output['predicted_label'].value_counts().reset_index()\n",
    "label_counts.columns = ['label', 'count']\n",
    "label_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1669a3a",
   "metadata": {},
   "source": [
    "**Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e02c61",
   "metadata": {},
   "source": [
    "- As conclusion we can say there are more genres than \"News\" in our dataset. So we have proved (by model) that on the information web (zdopravy.cz) there occur mainly news :-)\n",
    "- We find the above model very interesting and we would like to know how it works and analyses the given data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
